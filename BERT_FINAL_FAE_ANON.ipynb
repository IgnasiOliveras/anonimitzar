{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMurvu0RAPp6p67w01DAWHC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnasiOliveras/anonimitzar/blob/main/BERT_FINAL_FAE_ANON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oxVE_2CgloZ",
        "outputId": "080b7c52-07c4-42cf-964d-b910b6256c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "  0%|          | 0/6943 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 6943/6943 [09:49<00:00, 11.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesamiento completado en 589.33 segundos.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect deep_translator faker tqdm transformers torch pandas\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "from deep_translator import GoogleTranslator\n",
        "from faker import Faker\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "# Configuración inicial\n",
        "DetectorFactory.seed = 0\n",
        "fake_es = Faker(\"es_ES\")\n",
        "\n",
        "# Cargar modelo BERT para NER en español\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "\n",
        "def detectar_genero(nombre):\n",
        "    \"\"\"Detecta si un nombre es masculino o femenino según Faker\"\"\"\n",
        "    if nombre in fake_es.first_name_male():\n",
        "        return \"male\"\n",
        "    elif nombre in fake_es.first_name_female():\n",
        "        return \"female\"\n",
        "    return \"neutral\"\n",
        "\n",
        "\n",
        "def generar_nombre_con_palabras(original_name):\n",
        "    \"\"\"Genera un nombre falso con el mismo número de palabras y el mismo género que el original\"\"\"\n",
        "    palabras = original_name.split()\n",
        "    num_palabras = len(palabras)\n",
        "\n",
        "    # Determinar el género del primer nombre\n",
        "    genero = detectar_genero(palabras[0])\n",
        "\n",
        "    while True:\n",
        "        if num_palabras == 1:\n",
        "            nombre = fake_es.first_name_male() if genero == \"male\" else fake_es.first_name_female()\n",
        "        elif num_palabras == 2:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()}\"\n",
        "        else:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()} {fake_es.last_name()}\"\n",
        "\n",
        "        if len(nombre.split()) == num_palabras:\n",
        "            return nombre\n",
        "\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    resultados = ner_model(texto)\n",
        "    entidades = []\n",
        "\n",
        "    for ent in resultados:\n",
        "        if ent['entity_group'] == 'PER':\n",
        "            start = ent['start']\n",
        "            end = ent['end']\n",
        "            original = texto[start:end]\n",
        "\n",
        "            # Ajuste fino para capturar correctamente los espacios\n",
        "            while start > 0 and texto[start-1] != ' ':\n",
        "                start -= 1\n",
        "            while end < len(texto) and texto[end] != ' ':\n",
        "                end += 1\n",
        "\n",
        "            entidades.append((start, end, original.strip()))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "\n",
        "def traducir_y_anonimizar(texto):\n",
        "    \"\"\"Traduce y anonimiza manteniendo el número de palabras y el género\"\"\"\n",
        "    if not texto.strip():\n",
        "        return texto\n",
        "\n",
        "    # Traducción\n",
        "    try:\n",
        "        if len(texto) > 3 and detect(texto) != \"es\":\n",
        "            texto = GoogleTranslator(source=\"auto\", target=\"es\").translate(texto)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "\n",
        "    # Anonimizar nombres en \"Me llamo...\"\n",
        "    texto = re.sub(\n",
        "        r\"(Me llamo\\s+)([A-ZÁÉÍÓÚÑa-záéíóúñ]+(?:\\s+[A-ZÁÉÍÓÚÑa-záéíóúñ]+)*)\",\n",
        "        lambda match: match.group(1) + generar_nombre_con_palabras(match.group(2)),\n",
        "        texto,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Anonimizar números\n",
        "    texto = re.sub(r\"\\b\\d{9}\\b\", lambda _: fake_es.phone_number(), texto)\n",
        "    texto = re.sub(r\"\\b\\d{8}[A-Za-z]\\b\", lambda _: fake_es.ssn(), texto)\n",
        "\n",
        "    # Detección de entidades con BERT\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = generar_nombre_con_palabras(original)\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando traducción y anonimización.\"\"\"\n",
        "    row[\"body\"] = traducir_y_anonimizar(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "# Conectar a la base de datos\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Crear la tabla si no existe\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS mi_tabla (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            body TEXT,\n",
        "            secret TEXT,\n",
        "            direction TEXT,\n",
        "            createdAt TEXT,\n",
        "            OpenchannelAccountId INTEGER,\n",
        "            OpenchannelInteractionId INTEGER,\n",
        "            UserId INTEGER,\n",
        "            ContactId INTEGER,\n",
        "            AttachmentId INTEGER,\n",
        "            sentBy TEXT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # Cargar datos desde Excel (solo si es necesario)\n",
        "    df = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df_body = df[[\"id\", \"body\", \"direction\",\"createdAt\",\"UserId\", \"ContactId\"]].copy()\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df_body.to_dict(orient=\"records\")), total=len(df_body)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df_body = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df_body.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_body = df_body.dropna(subset=[\"UserId\"])"
      ],
      "metadata": {
        "id": "OmMTkPNAqgln"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 2 dataframes based on the variable direction in and out\n",
        "\n",
        "import pandas as pd\n",
        "# Create two dataframes based on the 'direction' column\n",
        "df_in = df_body[df_body['direction'] == 'in'].copy()\n",
        "df_out = df_body[df_body['direction'] == 'out'].copy()\n",
        "\n",
        "# Now you have two separate dataframes: df_in and df_out\n",
        "print(\"DataFrame 'in':\")\n",
        "print(df_in.head())  # Display the first few rows of df_in\n",
        "print(\"\\nDataFrame 'out':\")\n",
        "print(df_out.head()) # Display the first few rows of df_out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SzfGt0JspDw",
        "outputId": "af84cd0f-c95e-40d1-bcec-6335ecd61b8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame 'in':\n",
            "      id                 body direction           createdAt  UserId  ContactId\n",
            "0  77623                 Hola        in 2023-01-01 00:10:19   109.0        466\n",
            "2  77625    Me llamo Aránzazu        in 2023-01-01 00:10:37   109.0        466\n",
            "5  77628               Acepto        in 2023-01-01 00:12:13   109.0        466\n",
            "7  77630  Feliz año nuevo 🎆🎈🎊        in 2023-01-01 00:14:09   109.0        466\n",
            "8  77631                🥰🥰🥰🥰🥰        in 2023-01-01 00:14:17   109.0        466\n",
            "\n",
            "DataFrame 'out':\n",
            "       id                                               body direction  \\\n",
            "3   77626                                              Hola!       out   \n",
            "4   77627  Antes de empezar a chatear, es necesario que l...       out   \n",
            "6   77629  Gracias Graciana Soy maria jose, en que te pue...       out   \n",
            "10  77633  Feliz año nuevo! Que 2023 te traiga cosas buen...       out   \n",
            "13  77636                          des de donde me escribes?       out   \n",
            "\n",
            "             createdAt  UserId  ContactId  \n",
            "3  2023-01-01 00:11:12   109.0        466  \n",
            "4  2023-01-01 00:11:33   109.0        466  \n",
            "6  2023-01-01 00:13:42   109.0        466  \n",
            "10 2023-01-01 00:15:22   109.0        466  \n",
            "13 2023-01-01 00:17:58   109.0        466  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 20 lineas del dataframe df_body\n",
        "\n",
        "# Display the first 20 lines of the df_body DataFrame\n",
        "print(df_body.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebEK1-o1tqbE",
        "outputId": "a6770eb6-b927-4fab-bf47-1bbe801426a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id                                               body direction  \\\n",
            "0   77623                                               Hola        in   \n",
            "2   77625                                  Me llamo Aránzazu        in   \n",
            "3   77626                                              Hola!       out   \n",
            "4   77627  Antes de empezar a chatear, es necesario que l...       out   \n",
            "5   77628                                             Acepto        in   \n",
            "6   77629  Gracias Graciana Soy maria jose, en que te pue...       out   \n",
            "7   77630                                Feliz año nuevo 🎆🎈🎊        in   \n",
            "8   77631                                              🥰🥰🥰🥰🥰        in   \n",
            "9   77632  Siento sola por eso dame la mensaje para nuevo...        in   \n",
            "10  77633  Feliz año nuevo! Que 2023 te traiga cosas buen...       out   \n",
            "11  77634                                            Gracias        in   \n",
            "12  77635                                              🥰🥰🥰🥰🥰        in   \n",
            "13  77636                          des de donde me escribes?       out   \n",
            "14  77637                                       En Barcelona        in   \n",
            "15  77638                                Y soy de Bangladesh        in   \n",
            "16  77639              entiendo que estas sola en barcelona.       out   \n",
            "17  77640                     estas estudiando o trabajando?       out   \n",
            "18  77641               Nada solamente estudiando castellano        in   \n",
            "19  77642        Este nuevo año quiero nuevo cosa en mi vida        in   \n",
            "20  77643  bueno, a ver si poco a poco vas haciendo amias...       out   \n",
            "\n",
            "             createdAt  UserId  ContactId  \n",
            "0  2023-01-01 00:10:19   109.0        466  \n",
            "2  2023-01-01 00:10:37   109.0        466  \n",
            "3  2023-01-01 00:11:12   109.0        466  \n",
            "4  2023-01-01 00:11:33   109.0        466  \n",
            "5  2023-01-01 00:12:13   109.0        466  \n",
            "6  2023-01-01 00:13:42   109.0        466  \n",
            "7  2023-01-01 00:14:09   109.0        466  \n",
            "8  2023-01-01 00:14:17   109.0        466  \n",
            "9  2023-01-01 00:14:44   109.0        466  \n",
            "10 2023-01-01 00:15:22   109.0        466  \n",
            "11 2023-01-01 00:16:54   109.0        466  \n",
            "12 2023-01-01 00:16:57   109.0        466  \n",
            "13 2023-01-01 00:17:58   109.0        466  \n",
            "14 2023-01-01 00:24:45   109.0        466  \n",
            "15 2023-01-01 00:24:58   109.0        466  \n",
            "16 2023-01-01 00:25:27   109.0        466  \n",
            "17 2023-01-01 00:25:48   109.0        466  \n",
            "18 2023-01-01 00:27:51   109.0        466  \n",
            "19 2023-01-01 00:28:19   109.0        466  \n",
            "20 2023-01-01 00:28:27   109.0        466  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker # Install pyspellchecker instead of spellchecker\n",
        "!pip install langdetect deep_translator faker tqdm transformers torch pandas emoji\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "from deep_translator import GoogleTranslator\n",
        "from faker import Faker\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "import emoji\n",
        "from spellchecker import SpellChecker # Import SpellChecker from pyspellchecker instead of spellchecker\n",
        "\n",
        "# Configuración inicial\n",
        "DetectorFactory.seed = 0\n",
        "fake_es = Faker(\"es_ES\")\n",
        "spell = SpellChecker(language=\"es\")\n",
        "\n",
        "# Cargar modelo BERT para NER en español\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "# Diccionario para mantener consistencia de nombres inventados\n",
        "nombre_map = {}\n",
        "\n",
        "def detectar_genero(nombre):\n",
        "    \"\"\"Detecta si un nombre es masculino o femenino según Faker\"\"\"\n",
        "    if nombre in fake_es.first_name_male():\n",
        "        return \"male\"\n",
        "    elif nombre in fake_es.first_name_female():\n",
        "        return \"female\"\n",
        "    return \"neutral\"\n",
        "\n",
        "def generar_nombre_con_palabras(original_name):\n",
        "    \"\"\"Genera un nombre falso con el mismo número de palabras y el mismo género\"\"\"\n",
        "    palabras = original_name.split()\n",
        "    num_palabras = len(palabras)\n",
        "\n",
        "    # Si ya existe un nombre ficticio asignado, reutilizarlo\n",
        "    if original_name in nombre_map:\n",
        "        return nombre_map[original_name]\n",
        "\n",
        "    # Determinar género\n",
        "    genero = detectar_genero(palabras[0])\n",
        "\n",
        "    while True:\n",
        "        if num_palabras == 1:\n",
        "            nombre = fake_es.first_name_male() if genero == \"male\" else fake_es.first_name_female()\n",
        "        elif num_palabras == 2:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()}\"\n",
        "        else:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()} {fake_es.last_name()}\"\n",
        "\n",
        "        if len(nombre.split()) == num_palabras:\n",
        "            nombre_map[original_name] = nombre  # Guardar para mantener consistencia\n",
        "            return nombre\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    resultados = ner_model(texto)\n",
        "    entidades = []\n",
        "\n",
        "    for ent in resultados:\n",
        "        if ent['entity_group'] == 'PER':\n",
        "            start = ent['start']\n",
        "            end = ent['end']\n",
        "            original = texto[start:end]\n",
        "\n",
        "            # Ajuste fino para capturar correctamente los espacios\n",
        "            while start > 0 and texto[start-1] != ' ':\n",
        "                start -= 1\n",
        "            while end < len(texto) and texto[end] != ' ':\n",
        "                end += 1\n",
        "\n",
        "            entidades.append((start, end, original.strip()))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "def normalizar_texto(texto):\n",
        "    \"\"\"Convierte el texto a minúsculas, corrige errores ortográficos y elimina emojis\"\"\"\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = emoji.replace_emoji(texto, replace=\"\")  # Eliminar emojis\n",
        "    texto = re.sub(r\"[^A-Za-zÁÉÍÓÚáéíóúÑñ0-9.,!?;:\\s]\", \"\", texto)  # Eliminar caracteres extraños\n",
        "    texto = \" \".join([spell.correction(word) if spell.correction(word) else word for word in texto.split()])  # Corregir ortografía\n",
        "    return texto.strip()\n",
        "\n",
        "def traducir_y_anonimizar(texto):\n",
        "    \"\"\"Traduce y anonimiza manteniendo consistencia, número de palabras y género\"\"\"\n",
        "    if not texto.strip():\n",
        "        return texto\n",
        "\n",
        "    # Preprocesar texto antes de anonimizar\n",
        "    texto = normalizar_texto(texto)\n",
        "\n",
        "    # Traducción si es necesario\n",
        "    try:\n",
        "        if len(texto) > 3 and detect(texto) != \"es\":\n",
        "            texto = GoogleTranslator(source=\"auto\", target=\"es\").translate(texto)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "\n",
        "    # Anonimizar nombres en \"Me llamo...\"\n",
        "    texto = re.sub(\n",
        "        r\"(me llamo\\s+)([a-záéíóúñ]+(?:\\s+[a-záéíóúñ]+)*)\",\n",
        "        lambda match: match.group(1) + generar_nombre_con_palabras(match.group(2)),\n",
        "        texto,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Anonimizar números\n",
        "    texto = re.sub(r\"\\b\\d{9}\\b\", lambda _: fake_es.phone_number(), texto)\n",
        "    texto = re.sub(r\"\\b\\d{8}[A-Za-z]\\b\", lambda _: fake_es.ssn(), texto)\n",
        "\n",
        "    # Detección de entidades con BERT\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = generar_nombre_con_palabras(original)\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando traducción y anonimización.\"\"\"\n",
        "    row[\"body\"] = traducir_y_anonimizar(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "# Conectar a la base de datos\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Crear la tabla si no existe\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS mi_tabla (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            body TEXT,\n",
        "            secret TEXT,\n",
        "            direction TEXT,\n",
        "            createdAt TEXT,\n",
        "            OpenchannelAccountId INTEGER,\n",
        "            OpenchannelInteractionId INTEGER,\n",
        "            UserId INTEGER,\n",
        "            ContactId INTEGER,\n",
        "            AttachmentId INTEGER,\n",
        "            sentBy TEXT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # Cargar datos desde Excel (solo si es necesario)\n",
        "    df = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df_body = df[[\"id\", \"body\", \"direction\",\"createdAt\",\"UserId\", \"ContactId\"]].copy()\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df_body.to_dict(orient=\"records\")), total=len(df_body)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df_body = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df_body.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ok_2h0dRwuFp",
        "outputId": "0de317c4-f2b4-4521-9535-72f75b983d0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "  0%|          | 0/6943 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "  0%|          | 1/6943 [00:00<1:45:18,  1.10it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "  4%|▍         | 287/6943 [03:24<1:18:57,  1.41it/s]Process ForkPoolWorker-4:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-12b614aa1b40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesar_fila\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_body\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Convertir la lista de diccionarios de vuelta a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}