{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFmJAIyth6uJ6bwff2AhJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnasiOliveras/anonimitzar/blob/main/Copia_de_PASPAS_ANON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "id": "bl9DDFr3wpSh",
        "outputId": "d238db04-ba7b-46b1-866d-515cf22e3424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "  0%|          | 0/6943 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TokenClassificationPipeline' object has no attribute 'aggregation_strategy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-5-380b20e54439>\", line 62, in procesar_fila\n    row[\"body\"] = reemplazar_nombres(row[\"body\"])\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-5-380b20e54439>\", line 47, in reemplazar_nombres\n    entidades = detectar_entidades(texto)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<ipython-input-5-380b20e54439>\", line 26, in detectar_entidades\n    predictions = ner_model.postprocess(outputs, aggregation_strategy=ner_model.aggregation_strategy)\n                                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'TokenClassificationPipeline' object has no attribute 'aggregation_strategy'\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-380b20e54439>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesar_fila\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Convertir la lista de diccionarios de vuelta a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-380b20e54439>\u001b[0m in \u001b[0;36mprocesar_fila\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocesar_fila\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m\"\"\"Procesa una fila aplicando anonimización de nombres.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreemplazar_nombres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-380b20e54439>\u001b[0m in \u001b[0;36mreemplazar_nombres\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreemplazar_nombres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;34m\"\"\"Reemplaza nombres detectados por una marca genérica.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mentidades\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetectar_entidades\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mreplacements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-380b20e54439>\u001b[0m in \u001b[0;36mdetectar_entidades\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Obtener las predicciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregation_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mentidades\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TokenClassificationPipeline' object has no attribute 'aggregation_strategy'"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "import torch # Import the torch module\n",
        "\n",
        "# Cargar modelo NER y tokenizador\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-spanish-cased-finetuned-ner\")\n",
        "\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    # Tokenizar el texto con truncamiento y padding\n",
        "    inputs = tokenizer(texto, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
        "\n",
        "    # Realizar la inferencia\n",
        "    with torch.no_grad(): # Deshabilitar el cálculo de gradientes para la inferencia\n",
        "        outputs = ner_model.model(**inputs)\n",
        "\n",
        "    # Obtener las predicciones\n",
        "    predictions = ner_model.postprocess(outputs, aggregation_strategy=ner_model.aggregation_strategy)\n",
        "\n",
        "    entidades = []\n",
        "    for ent in predictions:\n",
        "        if ent['entity_group'] == 'PER':\n",
        "            start = ent['start']\n",
        "            end = ent['end']\n",
        "            original = texto[start:end]\n",
        "\n",
        "            # Ajustar límites de la entidad\n",
        "            while start > 0 and texto[start-1] not in (' ', '\\n', '\\t'):\n",
        "                start -= 1\n",
        "            while end < len(texto) and texto[end] not in (' ', '\\n', '\\t'):\n",
        "                end += 1\n",
        "\n",
        "            entidades.append((start, end, original.strip()))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "def reemplazar_nombres(texto):\n",
        "    \"\"\"Reemplaza nombres detectados por una marca genérica.\"\"\"\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = \"[NOMBRE]\"\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando anonimización de nombres.\"\"\"\n",
        "    row[\"body\"] = reemplazar_nombres(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Cargar datos del archivo Excel a un DataFrame\n",
        "    df_excel = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "\n",
        "    # Crear la tabla 'mi_tabla' a partir del DataFrame\n",
        "    df_excel.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df = pd.read_sql(\"SELECT id, body FROM mi_tabla\", conn)\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df.to_dict(orient=\"records\")), total=len(df)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "import torch  # Import the torch module\n",
        "\n",
        "# Cargar modelo NER y tokenizador\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    entidades = []\n",
        "\n",
        "    try:\n",
        "        predictions = ner_model(texto)\n",
        "        for ent in predictions:\n",
        "            if ent['entity_group'] == 'PER':\n",
        "                start, end = ent['start'], ent['end']\n",
        "                original = texto[start:end].strip()\n",
        "\n",
        "                # Ajustar los límites para capturar nombres completos\n",
        "                while start > 0 and texto[start-1] not in (' ', '\\n', '\\t'):\n",
        "                    start -= 1\n",
        "                while end < len(texto) and texto[end] not in (' ', '\\n', '\\t'):\n",
        "                    end += 1\n",
        "\n",
        "                entidades.append((start, end, original))\n",
        "    except Exception as e:\n",
        "        print(f\"Error en NER: {e}\")\n",
        "\n",
        "    return entidades\n",
        "\n",
        "def reemplazar_nombres(texto):\n",
        "    \"\"\"Reemplaza nombres detectados por una marca genérica.\"\"\"\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = \"[NOMBRE]\"\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando anonimización de nombres.\"\"\"\n",
        "    row[\"body\"] = reemplazar_nombres(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "# Conectar con SQLite\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Cargar datos del archivo Excel a un DataFrame\n",
        "    df_excel = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "\n",
        "    # Crear la tabla 'mi_tabla' a partir del DataFrame\n",
        "    df_excel.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "    conn.commit()  # Asegurar que los cambios se guarden\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df = pd.read_sql(\"SELECT id, body FROM mi_tabla\", conn)\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(max(cpu_count() - 1, 1)) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df.to_dict(orient=\"records\")), total=len(df)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "    conn.commit()  # Confirmar los cambios\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")\n"
      ],
      "metadata": {
        "id": "QCT5BdjCxR1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bec479-2550-4a51-fb5e-f77f2020bb63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            " 42%|████▏     | 2942/6943 [06:36<13:06,  5.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error en NER: The size of tensor a (630) must match the size of tensor b (512) at non-singleton dimension 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6943/6943 [15:24<00:00,  7.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesamiento completado en 925.09 segundos.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print all the columns of the db\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(\"SELECT name FROM PRAGMA_TABLE_INFO('mi_tabla')\")\n",
        "    columns = cursor.fetchall()\n",
        "    print(\"Columnas de la tabla 'mi_tabla':\")\n",
        "    for column in columns:\n",
        "column[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "rOP_217Ew7Hg",
        "outputId": "d0429a13-1597-4c94-d441-62fef38d9506"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 11 (<ipython-input-15-b3ddffa83698>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-b3ddffa83698>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    column[0]\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxOjIHBWw6Y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}