{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwODQQbXziK36wOTEFpwtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnasiOliveras/anonimitzar/blob/main/DEF_v1_BERT_BBDD_FAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViSSrVPV-S_H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect deep_translator faker tqdm transformers torch\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "from deep_translator import GoogleTranslator\n",
        "from faker import Faker\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# Configuración inicial\n",
        "DetectorFactory.seed = 0\n",
        "fake_es = Faker(\"es_ES\")\n",
        "\n",
        "# Cargar modelo BERT para NER en español\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "\n",
        "def generar_nombre_con_palabras(original_name):\n",
        "    \"\"\"Genera un nombre falso con el mismo número de palabras que el original\"\"\"\n",
        "    num_palabras = len(original_name.split())\n",
        "\n",
        "    # Generar nombres hasta que coincida el número de palabras\n",
        "    while True:\n",
        "        if num_palabras == 1:\n",
        "            nombre = fake_es.first_name()\n",
        "        elif num_palabras == 2:\n",
        "            nombre = f\"{fake_es.first_name()} {fake_es.last_name()}\"\n",
        "        else:\n",
        "            nombre = f\"{fake_es.first_name()} {fake_es.last_name()} {fake_es.last_name()}\"\n",
        "\n",
        "        if len(nombre.split()) == num_palabras:\n",
        "            return nombre\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    resultados = ner_model(texto)\n",
        "    entidades = []\n",
        "\n",
        "    for ent in resultados:\n",
        "        if ent['entity_group'] == 'PER':\n",
        "            start = ent['start']\n",
        "            end = ent['end']\n",
        "            original = texto[start:end]\n",
        "\n",
        "            # Ajuste fino para capturar correctamente los espacios\n",
        "            while start > 0 and texto[start-1] != ' ':\n",
        "                start -= 1\n",
        "            while end < len(texto) and texto[end] != ' ':\n",
        "                end += 1\n",
        "\n",
        "            entidades.append((start, end, original.strip()))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "def traducir_y_anonimizar(texto):\n",
        "    \"\"\"Traduce y anonimiza manteniendo el número de palabras\"\"\"\n",
        "    if not texto.strip():\n",
        "        return texto\n",
        "\n",
        "    # Traducción\n",
        "    try:\n",
        "        if len(texto) > 3 and detect(texto) != \"es\":\n",
        "            texto = GoogleTranslator(source=\"auto\", target=\"es\").translate(texto)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "\n",
        "    # Anonimizar nombres en \"Me llamo...\"\n",
        "    texto = re.sub(\n",
        "        r\"(Me llamo\\s+)([A-ZÁÉÍÓÚÑa-záéíóúñ]+(?:\\s+[A-ZÁÉÍÓÚÑa-záéíóúñ]+)*)\",\n",
        "        lambda match: match.group(1) + generar_nombre_con_palabras(match.group(2)),\n",
        "        texto,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Anonimizar números\n",
        "    texto = re.sub(r\"\\b\\d{9}\\b\", lambda _: fake_es.phone_number(), texto)\n",
        "    texto = re.sub(r\"\\b\\d{8}[A-Za-z]\\b\", lambda _: fake_es.ssn(), texto)\n",
        "\n",
        "    # Detección de entidades con BERT\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = generar_nombre_con_palabras(original)\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando traducción y anonimización.\"\"\"\n",
        "    row[\"body\"] = traducir_y_anonimizar(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "# Conectar a la base de datos\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Crear la tabla si no existe\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS mi_tabla (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            body TEXT,\n",
        "            secret TEXT,\n",
        "            direction TEXT,\n",
        "            createdAt TEXT,\n",
        "            OpenchannelAccountId INTEGER,\n",
        "            OpenchannelInteractionId INTEGER,\n",
        "            UserId INTEGER,\n",
        "            ContactId INTEGER,\n",
        "            AttachmentId INTEGER,\n",
        "            sentBy TEXT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # Cargar datos desde Excel (solo si es necesario)\n",
        "    df = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df_body = df[[\"id\", \"body\", \"direction\",\"createdAt\",\"UserId\", \"ContactId\"]].copy()\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df_body.to_dict(orient=\"records\")), total=len(df_body)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df_body = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df_body.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOFBU9Of4TLZ",
        "outputId": "213bb3d9-e7bc-435f-ef09-243a04eba489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "  0%|          | 0/6943 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "100%|██████████| 6943/6943 [14:37<00:00,  7.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesamiento completado en 878.13 segundos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect deep_translator faker tqdm transformers torch\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "from deep_translator import GoogleTranslator\n",
        "from faker import Faker\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "# Configuración inicial\n",
        "DetectorFactory.seed = 0\n",
        "fake_es = Faker(\"es_ES\")\n",
        "\n",
        "# Cargar modelo BERT para NER en español\n",
        "ner_model = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\", aggregation_strategy=\"simple\")\n",
        "\n",
        "\n",
        "def detectar_genero(nombre):\n",
        "    \"\"\"Detecta si un nombre es masculino o femenino según Faker\"\"\"\n",
        "    if nombre in fake_es.first_name_male():\n",
        "        return \"male\"\n",
        "    elif nombre in fake_es.first_name_female():\n",
        "        return \"female\"\n",
        "    return \"neutral\"\n",
        "\n",
        "\n",
        "def generar_nombre_con_palabras(original_name):\n",
        "    \"\"\"Genera un nombre falso con el mismo número de palabras y el mismo género que el original\"\"\"\n",
        "    palabras = original_name.split()\n",
        "    num_palabras = len(palabras)\n",
        "\n",
        "    # Determinar el género del primer nombre\n",
        "    genero = detectar_genero(palabras[0])\n",
        "\n",
        "    while True:\n",
        "        if num_palabras == 1:\n",
        "            nombre = fake_es.first_name_male() if genero == \"male\" else fake_es.first_name_female()\n",
        "        elif num_palabras == 2:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()}\"\n",
        "        else:\n",
        "            nombre = f\"{fake_es.first_name_male() if genero == 'male' else fake_es.first_name_female()} {fake_es.last_name()} {fake_es.last_name()}\"\n",
        "\n",
        "        if len(nombre.split()) == num_palabras:\n",
        "            return nombre\n",
        "\n",
        "\n",
        "def detectar_entidades(texto):\n",
        "    \"\"\"Detecta entidades PER usando BERT\"\"\"\n",
        "    resultados = ner_model(texto)\n",
        "    entidades = []\n",
        "\n",
        "    for ent in resultados:\n",
        "        if ent['entity_group'] == 'PER':\n",
        "            start = ent['start']\n",
        "            end = ent['end']\n",
        "            original = texto[start:end]\n",
        "\n",
        "            # Ajuste fino para capturar correctamente los espacios\n",
        "            while start > 0 and texto[start-1] != ' ':\n",
        "                start -= 1\n",
        "            while end < len(texto) and texto[end] != ' ':\n",
        "                end += 1\n",
        "\n",
        "            entidades.append((start, end, original.strip()))\n",
        "\n",
        "    return entidades\n",
        "\n",
        "\n",
        "def traducir_y_anonimizar(texto):\n",
        "    \"\"\"Traduce y anonimiza manteniendo el número de palabras y el género\"\"\"\n",
        "    if not texto.strip():\n",
        "        return texto\n",
        "\n",
        "    # Traducción\n",
        "    try:\n",
        "        if len(texto) > 3 and detect(texto) != \"es\":\n",
        "            texto = GoogleTranslator(source=\"auto\", target=\"es\").translate(texto)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "\n",
        "    # Anonimizar nombres en \"Me llamo...\"\n",
        "    texto = re.sub(\n",
        "        r\"(Me llamo\\s+)([A-ZÁÉÍÓÚÑa-záéíóúñ]+(?:\\s+[A-ZÁÉÍÓÚÑa-záéíóúñ]+)*)\",\n",
        "        lambda match: match.group(1) + generar_nombre_con_palabras(match.group(2)),\n",
        "        texto,\n",
        "        flags=re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    # Anonimizar números\n",
        "    texto = re.sub(r\"\\b\\d{9}\\b\", lambda _: fake_es.phone_number(), texto)\n",
        "    texto = re.sub(r\"\\b\\d{8}[A-Za-z]\\b\", lambda _: fake_es.ssn(), texto)\n",
        "\n",
        "    # Detección de entidades con BERT\n",
        "    entidades = detectar_entidades(texto)\n",
        "    replacements = []\n",
        "\n",
        "    for start, end, original in entidades:\n",
        "        fake_name = generar_nombre_con_palabras(original)\n",
        "        replacements.append((start, end, fake_name))\n",
        "\n",
        "    # Aplicar reemplazos en orden inverso\n",
        "    for start, end, fake_name in sorted(replacements, key=lambda x: -x[0]):\n",
        "        texto = texto[:start] + fake_name + texto[end:]\n",
        "\n",
        "    return texto\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def procesar_fila(row):\n",
        "    \"\"\"Procesa una fila aplicando traducción y anonimización.\"\"\"\n",
        "    row[\"body\"] = traducir_y_anonimizar(row[\"body\"])\n",
        "    return row\n",
        "\n",
        "# Conectar a la base de datos\n",
        "with sqlite3.connect(\"mi_base_de_datos.db\") as conn:\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Crear la tabla si no existe\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS mi_tabla (\n",
        "            id INTEGER PRIMARY KEY,\n",
        "            body TEXT,\n",
        "            secret TEXT,\n",
        "            direction TEXT,\n",
        "            createdAt TEXT,\n",
        "            OpenchannelAccountId INTEGER,\n",
        "            OpenchannelInteractionId INTEGER,\n",
        "            UserId INTEGER,\n",
        "            ContactId INTEGER,\n",
        "            AttachmentId INTEGER,\n",
        "            sentBy TEXT\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.commit()\n",
        "\n",
        "    # Cargar datos desde Excel (solo si es necesario)\n",
        "    df = pd.read_excel(\"MOSTRA_1.xlsx\")\n",
        "    df.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    # Extraer solo columnas relevantes\n",
        "    df_body = df[[\"id\", \"body\", \"direction\",\"createdAt\",\"UserId\", \"ContactId\"]].copy()\n",
        "\n",
        "    # Usar tqdm para mostrar progreso\n",
        "    start_time = time.time()\n",
        "    with Pool(cpu_count()) as pool:\n",
        "        result = list(tqdm(pool.imap(procesar_fila, df_body.to_dict(orient=\"records\")), total=len(df_body)))\n",
        "\n",
        "    # Convertir la lista de diccionarios de vuelta a DataFrame\n",
        "    df_body = pd.DataFrame(result)\n",
        "\n",
        "    # Actualizar base de datos en un solo paso eficiente\n",
        "    df_body.to_sql(\"mi_tabla\", conn, if_exists=\"replace\", index=False)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Procesamiento completado en {elapsed_time:.2f} segundos.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL23xJb1CBwz",
        "outputId": "d4674a83-8799-476c-aab7-aafdb01e900c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: deep_translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at mrm8488/bert-spanish-cased-finetuned-ner were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "  0%|          | 0/6943 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "  0%|          | 1/6943 [00:02<4:10:07,  2.16s/it]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            " 21%|██        | 1448/6943 [08:14<1:00:37,  1.51it/s]"
          ]
        }
      ]
    }
  ]
}